{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29b06892",
   "metadata": {},
   "source": [
    "# Introducción a Pytorch\n",
    "\n",
    "## Preparación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f42c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch torchvision torchaudio scikit-learn  # (si hace falta)\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(0)  # reproducibilidad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74546da",
   "metadata": {},
   "source": [
    "# Datos sintéticos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69295df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=0)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long)     # CrossEntropyLoss espera long\n",
    "X_val   = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val   = torch.tensor(y_val, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78a4b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=y, s=10)\n",
    "plt.title(\"Dos lunas (datos sintéticos)\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ff9622",
   "metadata": {},
   "source": [
    "# Propagación hacia adelante (forward)\n",
    "\n",
    "## 1. Forward “a mano” con tensores (ilustra el cálculo)\n",
    "\n",
    "Definimos pesos aleatorios y aplicamos: X -> capa oculta -> ReLU -> salida (logits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d5df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensiones\n",
    "in_features = 2\n",
    "hidden = 16\n",
    "out_features = 2  # dos clases\n",
    "\n",
    "# Inicialización aleatoria (por defecto ~N(0,1) escalado según init de PyTorch si usas nn.Linear;\n",
    "# aquí lo haremos explícito con torch.randn)\n",
    "W1 = torch.randn(in_features, hidden, dtype=torch.float32) * 0.1\n",
    "b1 = torch.zeros(hidden, dtype=torch.float32)\n",
    "\n",
    "W2 = torch.randn(hidden, out_features, dtype=torch.float32) * 0.1\n",
    "b2 = torch.zeros(out_features, dtype=torch.float32)\n",
    "\n",
    "# Forward manual (sin autograd por ahora, solo para ver el paso hacia adelante)\n",
    "def forward_manual(X):\n",
    "    z1 = X @ W1 + b1          # capa lineal 1\n",
    "    h1 = F.relu(z1)           # activación\n",
    "    logits = h1 @ W2 + b2     # capa lineal 2\n",
    "    return logits\n",
    "\n",
    "logits0 = forward_manual(X_train)     # [N, 2]\n",
    "print(\"Logits iniciales (primeras 5 filas):\\n\", logits0[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf6049d",
   "metadata": {},
   "source": [
    "Pérdida inicial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345180e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "loss0 = loss_fn(logits0, y_train)\n",
    "print(\"Pérdida inicial (sin entrenar):\", float(loss0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41c9ed",
   "metadata": {},
   "source": [
    "## 2. Forward usando un nn.Module (lo normal en PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472f66ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_features=2, hidden=16, out_features=2):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(in_features, hidden)  # inicializa pesos aleatoriamente\n",
    "        self.fc2 = nn.Linear(hidden, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)  # logits (sin softmax para usar CrossEntropyLoss)\n",
    "        return x\n",
    "\n",
    "net = MLP(in_features=2, hidden=16, out_features=2)\n",
    "logits = net(X_train)           # forward\n",
    "loss = loss_fn(logits, y_train) # pérdida inicial\n",
    "print(\"Pérdida inicial (nn.Module):\", float(loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb51c96c",
   "metadata": {},
   "source": [
    "# Propagación hacia atrás y optimización\n",
    "\n",
    "El ciclo de entrenamiento en PyTorch siempre sigue estos pasos:\n",
    "\n",
    "1. `optimizer.zero_grad()` — limpiar gradientes acumulados\n",
    "2. `logits = net(X_batch)` — **forward**\n",
    "3. `loss = loss_fn(logits, y_batch)` — calcular pérdida\n",
    "4. `loss.backward()` — **backward** (autograd calcula ∂loss/∂θ)\n",
    "5. `optimizer.step()` — **actualiza** parámetros con el optimizador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e584573f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9)  # también puedes usar Adam\n",
    "\n",
    "def accuracy(model, X, y):\n",
    "    with torch.no_grad():\n",
    "        pred = model(X).argmax(dim=1)\n",
    "        return (pred == y).float().mean().item()\n",
    "\n",
    "print(\"Acc (antes):\", accuracy(net, X_val, y_val))\n",
    "\n",
    "for epoch in range(200):\n",
    "    optimizer.zero_grad()\n",
    "    logits = net(X_train)\n",
    "    loss = loss_fn(logits, y_train)\n",
    "    loss.backward()          # calcula gradientes d(loss)/d(param)\n",
    "    optimizer.step()         # actualiza pesos\n",
    "\n",
    "    if (epoch+1) % 40 == 0:\n",
    "        acc_tr = accuracy(net, X_train, y_train)\n",
    "        acc_va = accuracy(net, X_val, y_val)\n",
    "        print(f\"Época {epoch+1:3d} | loss={loss.item():.4f} | acc_tr={acc_tr:.3f} | acc_val={acc_va:.3f}\")\n",
    "\n",
    "print(\"Acc (después):\", accuracy(net, X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90805b56",
   "metadata": {},
   "source": [
    "# Frontera de decisión visual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd155863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Malla para visualizar la frontera de decisión\n",
    "xx, yy = np.meshgrid(\n",
    "    np.linspace(X[:,0].min()-0.5, X[:,0].max()+0.5, 300),\n",
    "    np.linspace(X[:,1].min()-0.5, X[:,1].max()+0.5, 300)\n",
    ")\n",
    "grid = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "\n",
    "with torch.no_grad():\n",
    "    Z = net(grid).argmax(dim=1).reshape(xx.shape).numpy()\n",
    "\n",
    "plt.contourf(xx, yy, Z, alpha=0.3)\n",
    "plt.scatter(X_val[:,0], X_val[:,1], c=y_val, s=10)\n",
    "plt.title(\"Frontera de decisión (modelo entrenado)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6afa7dd",
   "metadata": {},
   "source": [
    "# Control exlpícito de la inicialización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb416392",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        nn.init.normal_(m.weight, mean=0.0, std=0.1)\n",
    "        nn.init.zeros_(m.bias)\n",
    "\n",
    "net = MLP()\n",
    "net.apply(init_weights)  # aplica tu init definida arriba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4510a23d",
   "metadata": {},
   "source": [
    "# Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210d9fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "X, y = make_moons(n_samples=1000, noise=0.2, random_state=0)\n",
    "\n",
    "# Control exlpícito de la inicialización\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Ejemplo de uso de Data loader de Pytorch\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train = torch.FloatTensor(X_train)\n",
    "y_train = torch.LongTensor(y_train)\n",
    "X_val = torch.FloatTensor(X_val)\n",
    "y_val = torch.LongTensor(y_val)\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "batch_size = 100\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445edeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "itertrain = iter(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c30f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "xbatch, ybatch = next(itertrain)\n",
    "print(xbatch.shape, ybatch.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rnap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
